{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lGw6pu2F1TY"
   },
   "source": [
    "# Deep Learning haciendo uso de CNN: ResNet\n",
    "---\n",
    "### Trabajo Final del curso Machine Learning\n",
    "### Docente: M.Sc. Richard Fernández \n",
    "\n",
    "**Integrantes**:\n",
    "- Lucas Parodi\n",
    "- Eduardo Contreras\n",
    "- Dante Moreno\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kjyNVnaF1Ti"
   },
   "source": [
    "## 1. Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-QEa4iTF1Tk"
   },
   "source": [
    "Deep Learning es un subcampo de Machine Learning, enfocado en algoritmos inspirados por la estructura y funcionalidad de las redes neuronales artificiales $^{[1]}$. El número de capas en una red neuronal es conocido como la profundidad de la red; y Deep Learning aprovecha la capacidad del hardware actual para utilizar redes neuronales más profundas que antes. En las últimas décadas, Deep Learning ha demostrado gran potencial gracias a su habilidad de manejar grandes volúmenes de data. $^{[2]}$\n",
    "\n",
    "El propósito de este trabajo es estudiar y aplicar Deep Learning utilizando redes convolucionales. Para esto, implementamos redes residuales, conocidas también como ResNet, en un caso de estudio. Luego, analizamos los resultados obtenidos para extraer conclusiones y recomendaciones pertinentes al caso de estudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiISQaUzF1Tl"
   },
   "source": [
    "## 2. Marco Teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8yVeSN_F1Tn"
   },
   "source": [
    "### 2.1. Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk7IFw1rF1Tp"
   },
   "source": [
    "En 2012, la visión computacional presenció un gran avance cuando un grupo de investigadores de la Universidad de Toronto desarrolló AlexNet, un sistema que se basaba en redes convolucionales artificiales, o CNN por sus siglas en inglés. $^{[2]}$ Este modelo superó a los mejores algoritmos de reconocimiento de imágenes por un amplio margen. Estas redes usan una técnica especial llamada Convolución, con la que reducen las imágenes a una forma que es más fácil de procesar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6J7cuWth1ioO"
   },
   "source": [
    "<img src='https://editor.analyticsvidhya.com/uploads/183560_qcMBDPuKpDvICcdd.png'>\n",
    "\n",
    "*Convolutional Neural Network* $^{[2]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gN9CKOfF1Tq"
   },
   "source": [
    "### 2.2. Aprendizaje Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8dkRxPvUF1Ts"
   },
   "source": [
    "El uso de redes neuronales convolucionales ha generado un gran avance en el estudio de visión computacional. Aunque crear redes más profundas, es decir, con más capas, podría parecer una forma de mejorar los resultados, en realidad vuelve más díficil el entrenamiento. Esto es llamado un problema de degradación: conforme la profundidad de la red aumenta, su resultado en accuracy disminuye rápidamente$^{[3]}$. La gradiente que se usa para entrenar a la red es propagada hacia atrás, y las multiplicaciones repetidas pueden hacer que esta gradiente se vuelve mínima$^{[4]}$. \n",
    "\n",
    "Los autores de ResNet reformularon el método tradicional de entrenar capas una tras otra. Según ellos: \"si se plantea la hipótesis de que múltiples capas no lineales pueden aproximarse asintóticamente a funciones complicadas, entonces es equivalente plantear la hipótesis de que pueden aproximarse asintóticamente a las funciones residuales\"$^{[3]}$. Es decir, no es necesario que las capas estén en una pila para optimizar la red.\n",
    "\n",
    "Para aplicar esto, se forman bloques residuales; donde existe una conección directa que salta alguna capas intermedias, llamada *skip connection*, que es el núcleo de estos bloques residuales$^{[5]}^$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://miro.medium.com/max/408/1*ByrVJspW-TefwlH7OLxNkg.png'>\n",
    "\n",
    "*Un bloque residual*$^{[4]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6hxKM98F1Tu"
   },
   "source": [
    "### 2.3. Redes Residuales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIoCHOXvF1Tw"
   },
   "source": [
    "<p>Una red neuronal residual, también conocida como ResNet, es una red neuronal artificial que se inspira en el hecho biológico de que algunas neuronas se conectan con neuronas en capas no necesariamente contiguas, saltando capas intermedias.</p>\n",
    "<img src='https://datascience.eu/wp-content/uploads/2020/09/image-27.png'>\n",
    "<p>Una razón de peso para saltar capas es evitar los gradientes que se desvanecen y cuestiones similares. Como el gradiente se retropropaga a las capas anteriores, este proceso repetido puede hacer que el gradiente sea extremadamente pequeño.</p>\n",
    "<p>El salto elimina las complicaciones de la red, haciéndola más simple, usando muy pocas capas durante la etapa de entrenamiento inicial. Acelera el aprendizaje por diez veces, minimizando el efecto de la desaparición de los gradientes. Después de esto, la red eventualmente vuelve a poner las capas hábiles mientras aprende el espacio de las características.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados de ResNet son evidentes. Mientras el enfoque tradicional de redes neuronales no mejora al agregar más capas por el problema de degradación, ResNet puede presentar cambios positivos con redes profundas.\n",
    "\n",
    "<img src='https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/09/09195100/0_AMK5ylLHQQ3CLQzk.png'>\n",
    "\n",
    "*Comparación de la evolución del error entre redes planas tradicionales y ResNet de 18 y 34 capas*$^{[5]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN8B_2H7F1Tx"
   },
   "source": [
    "## 3. Caso de Estudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset UISketch$^{[8]}$ contiene imágenes de bosquejos de diferentes elementos que pertenecen a la interfaz de usuario. El conjunto de datos contiene los atributos:\n",
    "* $name$: Nombre del archivo donde se guarda la imagen del bosquejo de elemento de UI.\n",
    "* $label$: Etiqueta con la que se identifica el tipo de elemento de UI.\n",
    "* $medium$: Medio con el cual se desarrolló el bosquejo; puede ser digital o papel.\n",
    "* $device$: Dispositivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Exploración de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el dataset *labels*, que consta de 19000 instancias de bosquejos de elementos de una interfaz de usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas:  19000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "      <th>medium</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alert/0000.jpg</td>\n",
       "      <td>alert</td>\n",
       "      <td>digital</td>\n",
       "      <td>stylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alert/0001.jpg</td>\n",
       "      <td>alert</td>\n",
       "      <td>digital</td>\n",
       "      <td>stylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alert/0002.jpg</td>\n",
       "      <td>alert</td>\n",
       "      <td>paper</td>\n",
       "      <td>pen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alert/0003.jpg</td>\n",
       "      <td>alert</td>\n",
       "      <td>digital</td>\n",
       "      <td>stylus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alert/0004.jpg</td>\n",
       "      <td>alert</td>\n",
       "      <td>paper</td>\n",
       "      <td>pen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  label   medium  device\n",
       "0  alert/0000.jpg  alert  digital  stylus\n",
       "1  alert/0001.jpg  alert  digital  stylus\n",
       "2  alert/0002.jpg  alert    paper     pen\n",
       "3  alert/0003.jpg  alert  digital  stylus\n",
       "4  alert/0004.jpg  alert    paper     pen"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/labels.csv')\n",
    "print(\"Filas: \", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los nombres de los archivos .jpg que guardan las imágenes, podemos leer también las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para agregar una columna de imágenes al dataset\n",
    "def add_images(data):\n",
    "    imgs = []\n",
    "    for r in data['name']:\n",
    "        img = cv2.imread('data/%s'%str(r))\n",
    "        imgs.append(img)\n",
    "    data[\"image\"] = imgs\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver, por ejemplo, un bosquejo que corresponde a la fila:\n",
    "\n",
    "label | medium | device\n",
    "--- | --- | ---\n",
    "alert | paper | pen\n",
    "\n",
    "Es decir, una alerta dibujada en papel, con un lápiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26debffd4e0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(cv2.imread('data/%s'%str(data['name'][2]))) # Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos examinar a profundidad las variables nominales de este conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label \t: 21 valores únicos\n",
      "medium \t: 2 valores únicos\n",
      "device \t: 6 valores únicos\n"
     ]
    }
   ],
   "source": [
    "columnas = data.columns\n",
    "for c in columnas[1:4]:\n",
    "    print(f'{c} \\t: {len(pd.unique(data[c]))} valores únicos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar las variables *medium* y *device* para analizar si existe alguna relación entre estas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medium  digital  paper\n",
      "device                \n",
      "finger       65      0\n",
      "mouse       131      0\n",
      "pen           0   4015\n",
      "pencil        0   1490\n",
      "stylus    13276      0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pd.crosstab(data[\"device\"],data[\"medium\"]))\n",
    "Medium = pd.crosstab(data[\"device\"],data[\"medium\"])\n",
    "Medium.div(Medium.sum(1).astype(float),axis=0).plot(kind=\"bar\",stacked=True,figsize=(4,4))\n",
    "plt.xlabel(\"Device\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable *medium* y *paper* están claramente relacionadas.\n",
    "Aquellos bosquejos realizados en papel fueron hechos con las herramientas esperadas: lápiz o lapicero. Por otro lado, el medio digital permite que los diseñadores usen otros dispositivos, como su dedo, mouse, o *stylus* (similar a un lápiz, para dibujos digitales).\n",
    "\n",
    "Además, notamos que se identificaron únicamente 5 valores únicos en *device*, por lo que el sexto valor señalado por la función $unique()$ podría ser el resultado de valores nulos. Para eso, vemos un resumen de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19000 entries, 0 to 18999\n",
      "Data columns (total 4 columns):\n",
      "name      19000 non-null object\n",
      "label     19000 non-null object\n",
      "medium    19000 non-null object\n",
      "device    18977 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 593.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La columna *device* tiene 23 datos vacíos. Por lo tanto, si deseamos usarla, necesitamos llenar esos datos.\n",
    "\n",
    "Entonces, elegimos como variable objetivo *medium*, sin datos faltantes, en lugar de *device*, donde sería redundante la información sobre el medio (digital o papel). Evaluamos la distribución de esta variable nominal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x26dee006dd8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(data['medium'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto de datos está desbalanceado, pues hay muchos más ejemplos de bosquejos hechos digitalmente que a mano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferencia entre Digital y Paper: 7990\n"
     ]
    }
   ],
   "source": [
    "n = len(data[data['medium'] == 'digital']) - len(data[data['medium'] == 'paper'])\n",
    "print(f'Diferencia entre Digital y Paper: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas con imágenes digitales: [0, 1, 3, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "digital_index = [i for i in data[data['medium'] == 'digital'].index]\n",
    "print('Filas con imágenes digitales:', digital_index[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De entre todas las instancias con bosquejos digitales, marcamos aleatoriamente 7990 para su eliminación, y así tener un conjunto de datos balanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se van a eliminar 7990 filas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[797, 6016, 8116, 13645, 17350]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_drop = random.sample(digital_index, n)\n",
    "print(f'Se van a eliminar {len(random_drop)} filas')\n",
    "random_drop[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(random_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11010"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, dado que solamente usaremos la columna de entrada (*image*) y la etiqueta *medium*, también soltamos las demás columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'medium'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(columns=['label', 'device'])\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardamos el DataFrame resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/data_under.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validamos que funcione:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11010\n",
      "Index(['name', 'medium'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/data_under.csv')\n",
    "print(len(data))\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for r in data['name']:\n",
    "    img = cv2.imread('data/%s'%str(r))\n",
    "    imgs.append(img)\n",
    "data[\"image\"] = imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11010,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['image'].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos de entrada serán las imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11010,)\n",
      "(11010,)\n",
      "['digital' 'paper' 'digital' 'paper' 'paper']\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.iloc[:,-1].values) #.to_numpy()\n",
    "y = np.array(data.iloc[:,1].values) #.to_numpy()\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lut = {'digital': 0, 'paper': 1}\n",
    "for i in range(len(y)):\n",
    "    y[i] = lut[y[i]]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para su ingreso en ResNet, necesitamos asegurar que $X$ sea considerado un arreglo de 4 dimensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11010, 224, 224, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X = np.array([np.array(x, dtype=np.uint8) for x in X], dtype=np.uint8)\n",
    "X = np.array([x for x in X])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11010, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.reshape((y.shape[0],1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Dividir en Training y Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 8808 imágenes, cada una de (224, 224, 3) dimensiones\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
    "\n",
    "print(f'Entrenamiento: {len(X_train)} imágenes, cada una de {X_train[0].shape} dimensiones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ta5-CMxFF1T3"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_vTwiiKtF1T5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: (224, 224, 3)\n",
      "Clases posibles: 2\n"
     ]
    }
   ],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "classes = 2\n",
    "\n",
    "print(f'Input Shape: {input_shape}')\n",
    "print(f'Clases posibles: {classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=input_shape,\n",
    "    pooling=None,\n",
    "    classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = torch.from_numpy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'uin16'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-0be9a5f77f2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_i\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muin16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 raise AttributeError(\"module {!r} has no attribute \"\n\u001b[1;32m--> 220\u001b[1;33m                                      \"{!r}\".format(__name__, attr))\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'uin16'"
     ]
    }
   ],
   "source": [
    "#y_train = np.array([int(y_i) for y_i in y_train], dtype=np.uin16)\n",
    "#y_train = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-d768f88d541e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    381\u001b[0m     dataset = dataset_ops.DatasetV2.zip((\n\u001b[0;32m    382\u001b[0m         \u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0mdataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m     ))\n\u001b[0;32m    385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    564\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m     \"\"\"\n\u001b[1;32m--> 566\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, element)\u001b[0m\n\u001b[0;32m   2763\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2764\u001b[0m     \u001b[1;34m\"\"\"See `Dataset.from_tensors()` for details.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2765\u001b[1;33m     \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2766\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2767\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\util\\structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[1;34m(element)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m           normalized_components.append(\n\u001b[1;32m--> 113\u001b[1;33m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[0;32m    114\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \"\"\"\n\u001b[0;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 258\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    264\u001b[0m   \u001b[0mctx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RUBzFthF1T6"
   },
   "source": [
    "### 3.1. Resultados del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PeVHmeXNF1T7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ES3uCxmLF1T8"
   },
   "source": [
    "## 4. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgOwAPp9YJAZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lOafM-oF1T9"
   },
   "source": [
    "## 5. Recomendaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KiPgNpeMF1T-"
   },
   "source": [
    "## Referencias bibliográficas:\n",
    "\n",
    "1. Brownlee, J. (16 de agosto de 2019). What is Deep Learning?. *Machine Learning Mastery* Recuperado de: https://machinelearningmastery.com/what-is-deep-learning/ \n",
    "2. Mandal, M. (1 de mayo de 2021). Introduction to Convolutional Neural Networks (CNN). *Analytics Vidha*. Recuperado de: https://www.analyticsvidhya.com/blog/2021/05/convolutional-neural-networks-cnn/\n",
    "3. He, K., Zhang, X., Ren, S., & Sun, J. (2015). *Deep Residual Learning for Image Recognition* Recuperado de: https://arxiv.org/abs/1512.03385\n",
    "4. Feng, V. (15 de julio de 2020). An Overview of ResNet and its Variants. *Towards Data Science*. Recuperado de: https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\n",
    "5. Mutjaba, H. (28 de septiembre de 2020). Introduction to Resnet or Residual Network. *Great Learning*. Recuperado de: https://www.mygreatlearning.com/blog/resnet/ \n",
    "6. Boesch, G. (29 de agosto de 2021). Deep Residual Networks (ResNet, ResNet50) – Guide in 2021 *Viso.ai*. Recuperado de: https://viso.ai/deep-learning/resnet-residual-neural-network/\n",
    "7. TensorFlow (12 de Noviembre de 2021) *Trabajar con capas de preprocesamiento* Recuperado de: https://www.tensorflow.org/guide/keras/preprocessing_layers\n",
    "8. Pandian, V. (2021). UISketch Dataset *Kaggle* Recuperado de: https://www.kaggle.com/vinothpandian/uisketch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4kjyNVnaF1Ti",
    "6gN9CKOfF1Tq",
    "eN8B_2H7F1Tx",
    "ES3uCxmLF1T8"
   ],
   "include_colab_link": true,
   "name": "TF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
